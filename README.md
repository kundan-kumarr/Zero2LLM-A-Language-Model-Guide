# 🧠 Large Language Models Bootcamp
## #90DaysLLMs [17th June - 25th Aug 2024]

A 90-day hands-on bootcamp designed to equip participants with practical skills in developing, deploying, and scaling applications powered by Large Language Models (LLMs). The program blends theory with real-world applications through capstone projects, live sessions, and tools like Hugging Face, CrewAI, LangChain, and more.

---

## 📚 Curriculum Overview

### 🔰 Practical Introduction to LLMs
- Applied LLM Foundations
- Real-World LLM Use Cases
- Domain and Task Adaptation Techniques

### 💡 Prompting and Prompt Engineering
- Basic Prompting Principles
- Prompting Types (Zero-shot, Few-shot, Chain-of-Thought, etc.)
- Applications, Risks, and Advanced Prompting Techniques

### 🔧 LLM Fine-Tuning
- Fine-Tuning Basics
- Instruction vs. Domain-Specific Fine-Tuning
- Fine-Tuning Challenges (Overfitting, Data Quality, etc.)

### 📦 Retrieval-Augmented Generation (RAG)
- Introduction to RAG
- Chunking, Embedding, and Vector Stores
- Advanced RAG Concepts: Ranking, Fusion, Guardrails, etc.

### 🛠️ Tools for Building LLM Applications
- Fine-Tuning Libraries (Hugging Face Transformers, PEFT)
- RAG Frameworks (Haystack, LlamaIndex, LangChain)
- Tooling for Observability, Prompt Management, Vector Search, and Deployment

### 📊 Evaluation Techniques
- Behavior vs. Performance Evaluation
- Common Benchmarks (HELM, MMLU, etc.)
- Metrics: BLEU, ROUGE, Perplexity, Accuracy, etc.

### 🧪 Build Your Own LLM App
- Define End-to-End LLM Workflow
- Build with Open Source Tools
- Deploy Using Free Resources (Gradio, Streamlit, Colab, etc.)

### 🚀 Advanced Features & Deployment
- LLM Lifecycle and LLMOps
- Scalable LLM Deployment (Docker, GitHub Actions)
- Monitoring, Ranking, and Explainability

### ⚠️ LLM Challenges
- Scaling and Memory Optimization
- Behavioral Issues (Hallucination, Alignment)
- Security and Deployment Risks

### 🔬 Emerging Research Trends
- Multimodal LLMs (Vision, Audio, Text)
- Open Source Innovations
- AI Agents and Agentic Workflows
- Novel Architectures (MoE, Mamba, RWKV, etc.)

### 🧠 LLM Foundations
- Generative Models Overview
- RNNs, LSTMs, Seq2Seq Models
- Transformers and Self-Attention Mechanism

---

## 🗓 Weekly Breakdown (Selected Highlights)

### 🏕 Week 0: Base Camp
- Python Refresher, Environment Setup
- FastAPI + Streamlit for LLM UI Prototyping

### 📖 Week 1–2: NLP + Hugging Face
- Data Handling, Tokenization
- Pretrained Model Fine-Tuning

### 💬 Week 3–5: LLMs, Chatbots & RAG
- Conversational Bot with Groq + LLaMA
- Chunking, Embedding, and Search (FAISS/QDrant)

### 🔎 Week 6–10: Multimodal + RAG with Tabular & Images
- RAG for Tables, Audio, Images, Video
- Summarization and Translation Tools

### 🧠 Week 11–16: Agentic AI with CrewAI, phiData, LangChain, Haystack, LlamaIndex
- Role-based Agents, Multi-Agent Workflows
- Custom Toolkits and Integration

### 🏗️ Week 17–24: Capstone Project
- Develop, Deploy and Scale your AI App
- CI/CD, Monitoring, Evaluation, Presentation

---

## 💼 Capstone Project

Each participant will:
- Design an end-to-end LLM-powered application.
- Integrate prompting, fine-tuning, RAG, and evaluation.
- Use modern frameworks and best practices.
- Present a production-ready AI solution at Demo Day.

---

## 🧑‍💻 Tools & Technologies

- Python, FastAPI, Streamlit, Gradio
- Hugging Face, Groq, CrewAI, LangChain, Haystack
- Docker, GitHub Actions, Jupyter, VS Code, Colab
- FAISS, QDrant, LanceDB

---

## 🤝 Community & Support

- Weekly live sessions with engineers & mentors
- Dedicated TAs for doubt resolution
- Monthly guest sessions by industry leaders
- Active peer-learning community

---

## 📬 Stay Updated

Follow the hashtag **#90DaysLLMs** on [LinkedIn](https://linkedin.com) and [Twitter](https://twitter.com) to see project highlights and learning milestones.

---

# 🧠 LLM-Crafter: Build Your Own Language Model

Welcome to LLM-Crafter — a step-by-step guide and implementation to build, train, and deploy a Language Model from scratch using PyTorch.

## 🚀 What You’ll Learn
- How tokenizers work
- How to prepare custom datasets
- How to implement GPT-style architectures
- Training strategies and optimization tricks
- Evaluating and fine-tuning LLMs
- Generating text and building inference pipelines

## 📂 Project Structure
- `01_tokenizer/`: Tokenizer from scratch (BPE or WordPiece)
- `02_dataset/`: Prepare your text data
- `03_model/`: GPT-style transformer architecture
- `04_training/`: Training loop, logging, early stopping
- `05_evaluation/`: Perplexity, BLEU, accuracy metrics
- `06_inference/`: Prompt-based inference pipeline
- `07_experiments/`: Logs and comparisons

## 🛠️ Setup

```bash
git clone https://github.com/yourusername/LLM-Crafter.git
cd LLM-Crafter
pip install -r requirements.txt


# LLM-Crafter 🚀
> Build, Train, Deploy your own LLM Applications from Scratch.


